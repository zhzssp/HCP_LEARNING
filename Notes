linear regression
1.计算图积累
2.随机数种子

softmax regression
1.累加器Accumulator
2.广播机制

多层感知机
1.收敛定理
2.sigmoid和tanh等涉及到指数运算的激活函数，自动求导的代价较高

模型调优
1.模型容量
2.权重衰退（最广泛的正则化方式），SGD提供该参数选择（1e-3或1e-4为主）
3.丢弃法（数据加入噪音，增强数据鲁棒性），对每个值扰动后的期望值仍为原数（0和xi/(1-p)），只对全连接层使用
4.让梯度在合理范围内，乘法变为加法（ResNet，LSTM），梯度归一化、裁剪，参数初始化，激活函数选取
5.将每一层的输出和梯度都看做随机变量，尽可能使期望和方差保持一致（nγ=1，Xavier初始化，n为输入或输出维度，γ为方差）
6.sigmoid * 4 - 2，调整后在0附近可以有较为稳定的期望和方差sigmoid(x) = x 
7.nan一般是除以0了，inf一般是梯度爆炸了（方差调小，学习率调小）
8.平移不变性（二维卷积/交叉相关），局部性（只在δ范围内设置参数，不关注里ij位置过远的特征），对全连接层使用平移不变性和局部性即得到卷积层

CNN
1.黑白通道为1，彩色通道为3
2.输出尺寸：(input_size - kernel_size + 2 * padding) / stride + 1
3.控制输出大小相等，2 * padding = kernel_size - 1（padding通常设成该大小）
4.数学上的卷积：对y[n] = x[n] * h[n]，对称->遍历k求和
5.不同通道用于提取不同的信息
6.窄但是深的神经网络效果显得更好（VGG块）
7.1*1卷积层约等于全连接层

batch normalization
1.顶部层快速收敛，底部层变化缓慢，不断地重新训练导致整体收敛较慢
2.作用在全连接层和卷积层的输出时，在激活函数前（方便在激活函数中落在0附近）；作用在全连接层和卷积层的输入上
3.对全连接层作用在特征维，对卷积层作用在通道维（像素可以理解为样本，通道为样本的特征）
4.加入随机偏移和缩放，控制模型复杂度（没必要跟dropout混合使用）
5.用于解决internal covariate shift（曾经使用降低lr）
6.只对batch数据映射到同一分布（batch_size需要够大）
7.均值和方差也会在反向传播时被考虑进去
8.加上γ和β参数可以使得映射到的分布更加灵活
9.testing stage只有一笔data进来，那么可以计算moving average，给靠近训练结束的部分更大的weight，从而更接近训练结束时的均值和方差
10.对参数的倍数变化不敏感

RNN
1.梯度爆炸：gradient clip
2.梯度消失：Gated RNN
