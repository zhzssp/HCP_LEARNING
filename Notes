linear regression
1.计算图积累
2.随机数种子

softmax regression
1.累加器Accumulator
2.广播机制

多层感知机
1.收敛定理
2.sigmoid和tanh等涉及到指数运算的激活函数，自动求导的代价较高

模型调优
1.模型容量
2.权重衰退（最广泛的正则化方式），SGD提供该参数选择（1e-3或1e-4为主）
3.丢弃法（数据加入噪音，增强数据鲁棒性），对每个值扰动后的期望值仍为原数（0和xi/(1-p)），只对全连接层使用
4.让梯度在合理范围内，乘法变为加法（ResNet，LSTM），梯度归一化、裁剪，参数初始化，激活函数选取
5.将每一层的输出和梯度都看做随机变量，尽可能使期望和方差保持一致（nγ=1，Xavier初始化，n为输入或输出维度，γ为方差）
6.sigmoid * 4 - 2，调整后在0附近可以有较为稳定的期望和方差sigmoid(x) = x 
7.nan一般是除以0了，inf一般是梯度爆炸了（方差调小，学习率调小）
8.平移不变性（二维卷积/交叉相关），局部性（只在δ范围内设置参数，不关注里ij位置过远的特征），对全连接层使用平移不变性和局部性即得到卷积层

CNN
1.黑白通道为1，彩色通道为3
2.输出尺寸：(input_size - kernel_size + 2 * padding) / stride + 1
3.控制输出大小相等，2 * padding = kernel_size - 1（padding通常设成该大小）
4.数学上的卷积：对y[n] = x[n] * h[n]，对称->遍历k求和
5.不同通道用于提取不同的信息
6.窄但是深的神经网络效果显得更好（VGG块）
7.1*1卷积层约等于全连接层

batch normalization
1.顶部层快速收敛，底部层变化缓慢，不断地重新训练导致整体收敛较慢
2.作用在全连接层和卷积层的输出时，在激活函数前（方便在激活函数中落在0附近）；作用在全连接层和卷积层的输入上
3.对全连接层作用在特征维，对卷积层作用在通道维（像素可以理解为样本，通道为样本的特征）
4.加入随机偏移和缩放，控制模型复杂度（没必要跟dropout混合使用）
5.用于解决internal covariate shift（曾经使用降低lr）
6.只对batch数据映射到同一分布（batch_size需要够大）
7.均值和方差也会在反向传播时被考虑进去
8.加上γ和β参数可以使得映射到的分布更加灵活
9.testing stage只有一笔data进来，那么可以计算moving average，给靠近训练结束的部分更大的weight，从而更接近训练结束时的均值和方差
10.对参数的倍数变化不敏感

RNN
1.梯度爆炸：gradient clip
2.梯度消失：Gated RNN、LSTM
3.BiRNN

目标检测：
1.边缘框。锚框
2.NMS：将类似的锚框去除(删除IoU值超过某个阈值的锚框)，Selective Search（融合相近的锚框）
3.区域卷积神经网络、选择性搜索、兴趣区域池化层（RoI，使锚框变为目标形状）、Fast RCNN、Faster RCNN（特别强调精度，速度较慢）、Mask RCNN（避免像素级预测时边界错位）
4.SSD（单发多框检测）：只需单神经网络，对每个像素生成多个锚框（类似RPN），每段都生成锚框（多尺度目标检测，在底层拟合小物体，在顶部拟合大物体），会产生大量的锚框重叠
5.YOLO：同样只需单神经网络，将图片均匀分成锚框，每个锚框预测复数个边缘框（圈中几个时都会进行预测）

transformer
1.解决问题：window无法开得过大，内存占用过大，overfitting
2.注意力机制：对每个输入向量生成query, key, value，通过query和key的内积得到相关度α，再以α为权重求和value得到输出向量
3.多头注意力机制：对每个query, key, value，再根据需要使用新的矩阵扩展出多个向量，以匹配不同方面的相关性，同一方面的query, key和value进行上述计算
4.注意力机制无法捕捉到位置信息（对不同位置的操作都是一样的），因此可以对每一个输入向量加上一个位置向量（使用位置编码的理论生成）
5.self attention可以认为包含了CNN，小批量数据适合使用CNN，大批量使用self attention会更好
6.与RNN对比：RNN对较远的信息感知较弱，并且其并非parallel，效率较低
7.self attention on gragh，只需要计算相连的节点就可以，这也是一种GNN
8.residual connection，直接将输入加到输出上
9.layer normalization（为什么不用batch normalization?）
10.masked self attention，不允许看后面的信息
11.decoder设置开始和结束符号（NA、NAT，后者往往表现比前者差due to multi-modality）
12.cross attention：从decoder输出获得query，并与encoder的输出向量计算相关性
13.每次产生输出都是一个分类问题（one-hot vector，注意输出还有end标志），最小化所有cross entropy的总和
14.Teacher Forcing，将正确答案作为decoder输入。可是在测试的时候无法获得正确答案
15.training tips：copy mechanism（将输入的部分直接复制到输出中，比如chat-bot或者摘要）、guided attention（monotonic attention, location-aware attention）
16.beam search束搜索，但是其在答案明确时较有用，需要创造性或者随机性时作用有限（不完美有时候更好）
17.BlEU Score（两个完整的句子作比较），与minimised cross entropy不一致，可以考虑在loss无法训练的时候使用RL硬做
18.exposure bias（一步预测错，步步错）：可选的解决方案schedule sampling（在训练时给decoder适当的错误输入，但是会伤害到并行性，在transformers中需要特别处理）
