def dropout_layer(x, dropout):
    assert 0 <= dropout <= 1
    if dropout == 1:
        return torch.zeros_like(x)
    if dropout == 0:
        return x
    mask = (torch.randn(x.shape) > dropout).float()
    return mask * x / (1.0 - dropout)


class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for block in args:
            self._modules[block] = block    #dictionary, special variable

    def forward(self, x):
        for block in self._modules.values():    #get the values of the dictionary
            x = block(x)
        return x

def convolution_2d(x, kernel):
    h, w = kernel.shape
    y = torch.zeros((x.shape[0] - h + 1, x.shape[1] - w + 1))
    for i in range(y.shape[0]):
        for j in range(y.shape[1]):
            y[i, j] = (x[i : i + h, j : j + w] * kernel).sum()
    return y

class Con2d(nn.Module):
    def __init__(self, kernel_size):
        super().__init__()
        self.weight = nn.Parameter(torch.rand(kernel_size))
        self.bias = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        return convolution_2d(x, self.weight) + self.bias

def batch_norm(x, gamma, beta, moving_mean, moving_var, eps, momentum):     #带moving的为由数据整体计算得到的量
    if not torch.is_grad_enabled():
        x_hat = (x - moving_mean) / torch.sqrt(moving_var + eps)       #推断模式
    else:
        assert len(x.shape) in (2, 4)    #判断是否是二维或者四维的tensor
        if len(x.shape) == 2:
            mean = x.mean(dim=0)    #按特征求平均数
            var = ((x - mean)**2).mean(dim=0)
        else:
            mean = x.mean(dim=(0, 2, 3), keepdim=True)
            var = ((x - mean)**2).mean(dim=(0, 2, 3), keepdim=True)
        x_hat = (x - mean) / torch.sqrt(var + eps)
        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean
        moving_var = momentum * moving_var + (1.0 - momentum) * var
    y = gamma * x_hat + beta
    return y, moving_mean, moving_var
