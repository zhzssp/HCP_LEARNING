import random
import torch

def synthetic_data(w, b, num):        #构造数据集
    x = torch.normal(0, 1, (num, len(w)))
    y = torch.matmul(x, w) + b
    y += torch.normal(0, 0.01, y.shape)
    return x, y.reshape((-1, 1))

true_w = torch.tensor([2, -3.4])            #准确w和b
true_b = 3.2
features, labels = synthetic_data(true_w, true_b, 1000)

def data_iter(batch_size, features, labels):
    num = len(features)
    indices = list(range(num))
    random.shuffle(indices)
    for i in range(0, num, batch_size):        #使用生成器返回batch_size的小批量features和labels
        batch_indices = torch.tensor(indices[i:min(i+batch_size, num)])
        yield features[batch_indices], labels[batch_indices]

batch_size = 10

for x, y in data_iter(batch_size, features, labels):#随机选取一个样本输出
    print(x, '\n', y)
    break;

w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)    #定义训练用初始参数w和b
b = torch.zeros(1, requires_grad=True)

def linreg(x, w, b):                #定义线性回归模型
    return torch.matmul(x, w) + b

def squared_loss(y_hat, y):    #以均方误差作为损失函数
    return (y_hat - y.reshape(y_hat.shape))**2 / 2

def gradient_descent(params, lr, batch_size):
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size    #损失函数处没有求均值，因此在此处求均值
            param.grad.zero_()    #手动调用函数使梯度清零

lr = 0.03
epochs = 10
net = linreg
loss = squared_loss

for epoch in range(epochs):            #训练
    for x, y in data_iter(batch_size, features, labels):
        l = loss(net(x, w, b), y)        #此时l为(batch_size, 1)而非标量，因此加和之后再求
        l.sum().backward()
        gradient_descent([w, b], lr, batch_size)
    with torch.no_grad():
        train_loss = loss(net(features, w, b), labels))    #显示数据集整体的loss
        print(f'epoch {epoch + 1}, loss {float(train_loss.mean()):f}')        #:f使得浮点数以小数格式输出，而非科学计数法

print(f'w_error {true_w - w.reshape(true_w.shape)}:f')
print(f'b_error {true_b - b}')
