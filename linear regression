import random
import torch

def synthetic_data(w, b, num):        #获取features和labels
    x = torch.normal(0, 1, (num, len(w)))
    y = torch.matmul(x, w) + b
    y += torch.normal(0, 0.01, y.shape)
    return x, y.reshape((-1, 1))

true_w = torch.tensor([2, -3.4])
true_b = 3.2
features, labels = synthetic_data(true_w, true_b, 1000)

def data_iter(batch_size, x, y):
    num = len(features)
    indices = list(range(num))
    random.shuffle(indices)
    for i in range(0, num, batch_size):        #使用生成器返回batch_size的features和labels
        batch_indices = torch.tensor(indices[i:min(i+batch_size, num)])
        yield features[batch_indices], labels[batch_indices]

batch_size = 10
for x, y in data_iter(batch_size, features, labels):#随机选取一个样本输出
    print(x, '\n', y)
    break;

w = torch.normal(0, 0.01, (2, 1), requires_grad=True)    #定义初始w和b
b = torch.zeros(1, requires_grad=True)
def linreg(x, w, b):                #定义线性回归层
    return torch.matmul(x, w) + b

def squared_loss(y_hat, y):    #以均方误差作为损失函数
    return (y_hat - y.reshape(y_hat.shape))**2 / 2

def optimizer(params, lr, batch_size):
    with torch.no_grad():
        for pram in params:
            para -= lr * param.grad / batch_size    #损失函数处没有求均值，因此在此处求均值
            para.grad.zero_()    #手动调用函数使梯度清零
